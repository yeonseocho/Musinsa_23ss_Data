#%%
import pandas as pd
import requests
from bs4 import BeautifulSoup as bs
import time
from tqdm import tqdm
#%%
url="https://www.musinsa.com/ranking/best?period=now&age=ALL&mainCategory=001&subCategory=&leafCategory=&price=&golf=false&kids=false&newProduct=false&exclusive=false&discount=false&soldOut=false&page=1&viewType=small&priceMin=&priceMax="
#%%
pd.read_html(url)

#%%
response=requests.get(url)
response.status_code
#%%
html=bs(response.text, 'lxml')
html
#%%
brand_html=html.select('#goodsRankList > li > div.li_inner > div.article_info > p.item_title > a')
brand_html[0].string
#%%
brand_list=[]
for i in brand_html:
    brand_list.append(i.string)
brand_list[0:5]
#%%
html.select('#goodsRankList > li:nth-child(1) > div.li_inner > div.article_info > p.list_info > a')
#%%
a=html.select('#goodsRankList > li:nth-child(1) > div.li_inner > div.article_info > p.list_info > a')
a[0]['href']
#%%
link_name_html=html.select('#goodsRankList > li > div.li_inner > div.article_info > p.list_info > a')
link_list=[]
name_list=[]
for i in link_name_html:
    link_list.append(i['href'])
    name_list.append(i['title'])
link_list[0:5], name_list[0:5]
#%%
rank_html=html.select('#goodsRankList > li > p')
rank_no_list=[]
for i in rank_html:
    rank_no_list.append(i.string.strip())
rank_no_list[0:5]
#%%
import selenium
from selenium import webdriver as wd
import time
import pandas as pd
from bs4 import BeautifulSoup
import requests

from itertools import repeat
import re
#%%
url = 'https://www.musinsa.com/brands/musinsastandard?category3DepthCodes=&category2DepthCodes=&category1DepthCode=&colorCodes=&startPrice=&endPrice=&exclusiveYn=&includeSoldOut=&saleGoods=&timeSale=&includeKeywords=&sortCode=emt_high&tags=&page=1&size=90&listViewType=small&campaignCode=&groupSale=&outletGoods=false&boutiqueGoods='
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')

# soup에 해당 url 페이지의 parsing된 html 정보 모두 담기게 됨"
#%%
soup.find_all('a', attrs={'class':'img-block'})
#%%
soup.find_all('a', attrs={'class':'img-block'})[0]['href'] # url
soup.find_all('a', attrs={'class':'img-block'})[0]['title'] # 상품제목
#%%
soup.find_all('p', attrs={'class':'price'})[0].get_text()
#%%
soup.find_all('p', attrs={'class':'price'})[0].get_text().split()[0]
#%%
[re.sub(r'[^0-9]', '', soup.get_text().split()[0]) for soup in soup.find_all('p', attrs={'class':'price'})]
#%%
start = 1
title_list = []
url_list = []
# price_list = []

while start<37: #1페이지~36페이지
    try:
        url = 'https://www.musinsa.com/brands/musinsastandard?category3DepthCodes=&category2DepthCodes=&category1DepthCode=&colorCodes=&startPrice=&endPrice=&exclusiveYn=&includeSoldOut=&saleGoods=&timeSale=&includeKeywords=&sortCode=emt_high&tags=&page={}&size=90&listViewType=small&campaignCode=&groupSale=&outletGoods=false&boutiqueGoods='.format(start)
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'lxml')

        for soup in soup.find_all('a', attrs={'class':'img-block'}):
            title_list.append(soup['title'])
            url_list.append('https:' + soup['href'])

        start += 1

    except:
        print(start)
        break
#%%
df = pd.DataFrame({'상품명': title_list,
                   'url': url_list})
df.to_csv('무신사.csv', encoding = 'utf-8-sig')